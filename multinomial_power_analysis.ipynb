{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multinomial Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions in `scipy.stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sp.stats.multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working primarily with two methods of `scipy.stats.multinomial`: `pmf()`, and `rvs()`,\n",
    "as well as a small set of parameters with consistent naming between the two methods: `x`, `n`, `p`, and `size`.\n",
    "\n",
    "To maintain clarity, I'll always try to use the same number of dimensions for the inputs and outputs of\n",
    "these functions, but it's worth noticing that distributions in \n",
    "`scipy.stats` do some tricky stuff with dimensions just to mess with you.\n",
    "\n",
    "- `x` : The observed counts in all $k$ categories.\n",
    "- `n` : The total number of samples taken.\n",
    "- `p` : The expected fraction of each category.\n",
    "- `size` : the number of trials: repeating the random draws with the same parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.rvs()` draws values from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.array(100)\n",
    "p = np.array([0.1, 0.5, 0.4])\n",
    "size = 10\n",
    "\n",
    "x = sp.stats.multinomial.rvs(n, p, size)\n",
    "print(x)\n",
    "\n",
    "# EXERCISE: Mess with the parameters and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.rvs()` calculates the probability of drawing a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "p = [1/3, 1/3, 1/3]\n",
    "x_diff = [15, 15, 0]\n",
    "x_same = [30, 0, 0]\n",
    "\n",
    "print(sp.stats.multinomial.pmf(x_diff, n, p))\n",
    "print(sp.stats.multinomial.pmf(x_same, n, p))\n",
    "\n",
    "# EXERCISE: Mess with the data and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like, `R`, `scipy.stats` takes pains to be as consistent as possible across methods and distributions.\n",
    "Fortunately that means you usually only have to learn weird quirks of the API once.\n",
    "\n",
    "EXERCISE: What happens when `np.sum(p) != 1.0`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNA composition as a multinomial model\n",
    "\n",
    "Let's think about how to translate a real system into a simple probabilistic model.\n",
    "\n",
    "A single strand of DNA is composed of $n$ positions, each of which may be in one of\n",
    "four states: $\\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$.\n",
    "While evolutionary selection potentially acting on this sequence means that any one position\n",
    "is not randomly assigned a base, our ignorance of this fact means that at\n",
    "larger scales the composition of bases is akin to repeatedly flipping a 4-sided coin\n",
    "... I mean rolling a 4-sided die ... or whatever.\n",
    "\n",
    "How do we relate the ideas in this abstract biological system to the parameters of the\n",
    "multinomial?\n",
    "\n",
    "EXERCISE: What do each of the variables represent?\n",
    "\n",
    "```python\n",
    "n = None  # ???\n",
    "p = None  # ???\n",
    "size = None  # ???\n",
    "x = None  # ???\n",
    "```\n",
    "\n",
    "EXERCISE: What scenario in the context above does the following model describe?\n",
    "\n",
    "```python\n",
    "sp.stats.multinomial.rvs(100, [1/4, 1/4, 1/4, 1/4], size=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The test statistics\n",
    "\n",
    "Much of frequentist statistics is about understanding how \"surprising\" a result is.\n",
    "This is exactly how a p-value should be interpretted: \"we would get this 'extreme'\n",
    "of a result only $p$ portion of the time under the null hypothesis.\n",
    "\n",
    "The first step in this process is deciding what we mean by \"extreme\".\n",
    "We'll need to choose a metric (formula) that calculates\n",
    "an extreme-ness score for our data, with larger values being more extreme/surprising.\n",
    "This formula is called a \"test statistic\" and for multivariate data\n",
    "(like our nucleotide base counts) it also has the benefit of\n",
    "being 1-dimensional rather than $k$-dimensional.\n",
    "\n",
    "You should think of test statistics as distances, they are (usually?) in the set\n",
    "$[0, \\inf]$ with larger values being more surprising.\n",
    "Sometimes we can parameterize our test statistic with a hypothesis\n",
    "(usually the null), but sometimes we can't and the hypothesis is\n",
    "implicit.\n",
    "\n",
    "For multinomial-like data, the sum of relative squared differences from the expectation\n",
    "is a natural (whatever that means) test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_relative_squared_differences(x, n, p):\n",
    "    expect = p * n\n",
    "    return np.sum((expect - x)**2 / expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "p = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "x_diff = np.array([1, 1, 0, 0])\n",
    "x_same = np.array([2, 0, 0, 0])\n",
    "\n",
    "print(sum_of_relative_squared_differences(x_diff, n, p))\n",
    "print(sum_of_relative_squared_differences(x_same, n, p))\n",
    "\n",
    "# EXERCISE: Play around with _which_ bases we hit under each scenario\n",
    "# (e.g. what if we hit both C's instead of both A's),\n",
    "# and see how it affects the test statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our statistic is telling us that it's more \"extreme\" for both positions\n",
    "to have the same base than it is for them to have different bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The null hypothesis\n",
    "\n",
    "Before we go any further, we should be sure to state what we mean by \"surprising\".\n",
    "This depends on what we expect.\n",
    "In a hand-wavy sort of way, what we expect is often described as the \"null\n",
    "hypothesis\" in frequentist statistics.  (I say hand-wavy, because there\n",
    "much of the time we don't _really_ expect the null, but that's a very different\n",
    "discussion.)\n",
    "\n",
    "The test statistic that we've selected for the multinomial (`sum_of_relative_squared_differences`)\n",
    "is pretty spiffy because we can parameterize it with whatever null hypothesis we want.\n",
    "Usually that's uniform across all of the options, $p_i = p_j$ for all $i$ and all $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "p = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "x_diff = np.array([1, 1, 1, 1])\n",
    "x_same = np.array([2, 1, 1, 0])\n",
    "\n",
    "print(sum_of_relative_squared_differences(x_diff, n, p))\n",
    "print(sum_of_relative_squared_differences(x_same, n, p))\n",
    "\n",
    "# EXERCISE: What happens to the test statistics under each scenario when you change\n",
    "# the expected proportion, `p`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing\n",
    "\n",
    "Okay, so we've concluded that under the uniform model described above,\n",
    "it's surprising to get lot's more of one base than the others.\n",
    "\n",
    "The next question is \"how surprising?\"\n",
    "\n",
    "For a p-value threshold of 0.05, what we are saying is:\n",
    "\n",
    "> if our test statistic is more extreme than what we'd expect 95% of\n",
    "> the time when our null hypothesis is true, then we decide it probably\n",
    "> isn't and \"reject\" it.\n",
    "\n",
    "So what we need to test our null hypothesis is a way to\n",
    "calculate the probability of seeing a more extreme value under the null.\n",
    "Notice that we're talking about our test statistic as though _it_ is being\n",
    "drawn from a random distribution.\n",
    "\n",
    "And it is;\n",
    "since it's a function of our data which is drawn from a probability distribution,\n",
    "the test statistic is itself drawn from a probability distribution.\n",
    "\n",
    "But what distribution?\n",
    "\n",
    "Sometimes we know, and this is one of those cases:\n",
    "under the null hypothesis and given some assumptions\n",
    "the sum of relative squared differences\n",
    "is well approximated by draws from the $\\chi^2$ distribution under the\n",
    "(with $k - 1$ degrees of freedom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obs = np.array([25, 25, 25, 25])\n",
    "\n",
    "p_null = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "n = np.sum(x_obs)\n",
    "df = p.shape[0] - 1\n",
    "\n",
    "statistic = sum_of_relative_squared_differences(x_obs, n, p_null)\n",
    "# The probability of a more extreme value is the complement of the probability of a\n",
    "# less extreme value.\n",
    "pvalue = 1 - sp.stats.chi2.cdf(statistic, df)\n",
    "print(pvalue)\n",
    "\n",
    "# EXERCISE: What happens to the p-value as you change the data, `x`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating data, statistics, p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the multinomial is handy because we've got a natural test statistic\n",
    "with a well understood distribution,\n",
    "but what do we do when we don't know the exact distribution of our test statistic?\n",
    "\n",
    "There are plenty of things we _can_ do, but maybe the most obvious is to\n",
    "simulation data from the null hypothesis and compare our observed test\n",
    "statistic to the simulated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_null = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "n = 1000\n",
    "size = 10000\n",
    "\n",
    "x_sim = sp.stats.multinomial.rvs(n, p_null, size=size)\n",
    "\n",
    "statistic_sim = np.apply_along_axis(\n",
    "    lambda y: sum_of_relative_squared_differences(y, n, p_null),\n",
    "    axis=1,\n",
    "    arr=x_sim\n",
    "                                    )\n",
    "\n",
    "plt.hist(statistic_sim, bins=100, density=True)\n",
    "# Add the chi2 PDF for comparison\n",
    "ss = np.linspace(0, 20, num=1000)\n",
    "df = p.shape[0] - 1\n",
    "plt.xlabel('SSD')\n",
    "plt.plot(ss, sp.stats.chi2.pdf(ss, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare our observed test statistic to arrive at a p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obs = np.array([290, 210, 250, 250])\n",
    "assert sum(x_obs) == n\n",
    "statistic_obs = sum_of_relative_squared_differences(x_obs, n, p_null)\n",
    "\n",
    "plt.hist(statistic_sim, bins=100, density=True)\n",
    "plt.axvline(statistic_obs, lw=1, color='k')\n",
    "plt.xlabel('SSD')\n",
    "print((statistic_sim > statistic_obs).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the same logic that we concluded the test statistic is randomly distributed,\n",
    "the p-value is also randomly distributed due to its relationship with the random\n",
    "data.\n",
    "\n",
    "Let's simulate a bunch of different data and see what this p-value distribution\n",
    "looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_null = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "n = 1000\n",
    "size = 10000\n",
    "\n",
    "x_sim = sp.stats.multinomial.rvs(n, p_null, size=size)\n",
    "df = p.shape[0] - 1\n",
    "\n",
    "statistic_sim = np.apply_along_axis(\n",
    "    lambda y: sum_of_relative_squared_differences(y, n, p_null),\n",
    "    axis=1,\n",
    "    arr=x_sim\n",
    "                                    )\n",
    "# I'm going back to the analytical distribution here, but you could\n",
    "# sub in the null distribution produced through simulation.\n",
    "pvalue_sim = 1 - sp.stats.chi2.cdf(statistic_sim, df)\n",
    "\n",
    "plt.hist(pvalue_sim, bins=20)\n",
    "plt.axvline(0.05, lw=1, color='r', linestyle='--')\n",
    "plt.xlabel('p')\n",
    "print((pvalue_sim < 0.05).mean())\n",
    "\n",
    "# EXERCISE: Why is the p-value distributed as it is under the null?\n",
    "\n",
    "# EXERCISE: Why is the value of `n` set to such a large value?\n",
    "# What happens when it's set to 100? 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that about 5% of the time our p < 0.05 ... under the null!\n",
    "\n",
    "These are false positives; $p < \\alpha$\n",
    "a fraction of the time equal to $\\alpha$.\n",
    "\n",
    "But what happens when the simulated data doesn't come from the null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_null = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "p_sim = np.array([0.27, 0.23, 0.25, 0.25])\n",
    "n = 1000\n",
    "size = 10000\n",
    "\n",
    "x_sim = sp.stats.multinomial.rvs(n, p_sim, size=size)\n",
    "df = p.shape[0] - 1\n",
    "\n",
    "statistic_sim = np.apply_along_axis(\n",
    "    lambda y: sum_of_relative_squared_differences(y, n, p_null),\n",
    "    axis=1,\n",
    "    arr=x_sim\n",
    "                                    )\n",
    "pvalue_sim = 1 - sp.stats.chi2.cdf(statistic_sim, df)\n",
    "\n",
    "plt.hist(pvalue_sim, bins=20)\n",
    "plt.axvline(0.05, lw=1, color='r', linestyle='--')\n",
    "plt.xlabel('p')\n",
    "print((pvalue_sim < 0.05).mean())\n",
    "\n",
    "# EXERCISE: What happens when you mess with the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we get far more p-values < 0.05, we therefore reject the null\n",
    "a larger fraction of the time, and since we _know_ that the null\n",
    "is wrong (our data are simulated from a non-uniform distribution),\n",
    "our \"hits\" are true positives.\n",
    "\n",
    "On the other hand, we also have plenty of trials where we did not\n",
    "reject the null hypothesis.  These are now false negatives.\n",
    "\n",
    "The fraction of trials that reject the null hypothesis is our\n",
    "study's \"power\".\n",
    "\n",
    "EXERCISE: What parameters does this power depend on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power analysis\n",
    "\n",
    "Let's put it all together into a power analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(p, n, alpha=0.05, p_null=None, nsim=1000):\n",
    "    k = p.shape[0]\n",
    "    if p_null is None:\n",
    "        p_null = np.ones_like(p) / k\n",
    "        \n",
    "    x_sim = sp.stats.multinomial.rvs(n, p, size=nsim)\n",
    "    df = k - 1\n",
    "    statistic_sim = np.apply_along_axis(\n",
    "        lambda y: sum_of_relative_squared_differences(y, n, p_null),\n",
    "        axis=1,\n",
    "        arr=x_sim\n",
    "                                        )\n",
    "    # If using an emprical null distribution, you'd probably\n",
    "    # want to pre-calculte the CDF, since you'll often call\n",
    "    # power() multiple times.\n",
    "    pvalue_sim = 1 - sp.stats.chi2.cdf(statistic_sim, df)\n",
    "    return (pvalue_sim < alpha).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's run it on the same data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_null = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "p_sim = np.array([0.27, 0.23, 0.25, 0.25])\n",
    "n = 1000\n",
    "\n",
    "print(power(p_sim, n))\n",
    "\n",
    "# EXERCISE: What happens each time you re-run the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are sometimes analytical solutions for the power, here we're using simulation.\n",
    "As the study design (and therefore the data distribution) gets more complicated, simulation\n",
    "will quickly become the only option.\n",
    "\n",
    "Conditional on an expected fractions of each base\n",
    "we can now calculate our power at varying sample sizes,\n",
    "$n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sim = np.array([0.27, 0.23, 0.25, 0.25])\n",
    "alpha = 0.05\n",
    "nsim = 1000\n",
    "\n",
    "nn = np.logspace(1, 4, num=50).astype(int)\n",
    "p_reject = [power(p_sim, n, alpha=alpha, nsim=nsim) for n in nn]\n",
    "\n",
    "plt.scatter(nn, p_reject, marker='.')\n",
    "plt.axhline(alpha, linestyle='--', lw=1, color='k')\n",
    "plt.axhline(0.8, linestyle='--', lw=1, color='k')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('power')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've found is that if we want a 80% chance of rejecting the null hypothesis\n",
    "we need a much larger sample size.\n",
    "\n",
    "Also notice the diminishing returns at high values of $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it.\n",
    "\n",
    "We've done it.\n",
    "\n",
    "We now know how to use simulation to calculate\n",
    "the power of our multinomial test for a given\n",
    "value of the underlying parameter $p$, with a given\n",
    "sample size $n$ and p-value threshold $\\alpha$.\n",
    "\n",
    "For other scenarios the procedure is the same:\n",
    "\n",
    "1. Pick a data generating distribution\n",
    "2. Pick a test statistic measuring the \"extremeness\" of the data: $\\phi(\\cdot)$\n",
    "3. Figure out (or simulate) the distribution of this test statistic under the null in order to calculate how surprising observed data is (a p-value): $1 - \\mathrm{CDF}_{\\mathrm{\\phi(x_{null})}}(\\phi(x))$\n",
    "4. Simulate the distribution of this test statistic under hypothetical parameters\n",
    "4. Calculate the power given your choice of parameters: $\\mathrm{Pr}(1 - \\mathrm{CDF}_{\\mathrm{null}}(\\phi(x)) < \\alpha)$\n",
    "\n",
    "For many simple distributions these steps have already been worked out for you,\n",
    "but often they have not been.\n",
    "In that case, simulation is a powerful tool.\n",
    "\n",
    "As a result, the challenge is usually picking \"realistic\" parameters for\n",
    "the data generating process, rather than calculating the power itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
